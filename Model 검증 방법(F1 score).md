# Model 검증 방법(F1 score)

> 감정 분류 모델을 검증하기 위한 방법 중 하나인 F1 score에 대해 정리합니다. F1 score를 구하는 식은 아래와 같습니다.



## 0. 들어가기

 앞서 말했듯이 F1 score는 분류 모델, classification 문제를 풀기 위한 모델을 평가하는 지표입니다. 우리의 모델은 얼굴 이미지를 받아들여 감정을 분류하는 모델로써, `input` 데이터를 기쁨(positive)/무표정(neutral)/찡그림(negative) 세 가지 중 하나로 분류한 `output`을 내보냅니다. 

 이러한 모델을 검증하기 위해서는 어떤 방법이 필요할까요? F1 score는 어떤 것이길래 이 모델의 성능을 측정할 수 있다는 것일까요? 우선 `F1 score`의 식부터 확인해 보겠습니다.  

![C13314_01_33](https://user-images.githubusercontent.com/58945760/81763078-4dc19d00-9509-11ea-99a5-0bf86fe30435.jpg)



 당연히 이해가 안 됩니다. 당최 어떻게 모델의 성능을 측정할 수 있다는 것인지 머릿속에 들어오질 않습니다.

 그래도 곱셈과 나눗셈은 알겠는데, 도무지 의미를 알 수 없는 저 알파벳들이 문제입니다.  `Precision`은 뭐고 `Recall`은 무엇일까요? 확실한 건, F1 score가 뭔지 알기 위해서는 이 두 가지를 이해해야 한다는 것입니다. 그럼 차근 차근 가보도록 하겠습니다. 아래의 그림을 봐 주세요.



![전체 식](https://user-images.githubusercontent.com/58945760/81879352-7b6c1c00-95c5-11ea-86b4-4c2ed11df9f5.png)

 사실 이 그림 한 장만 이해하면 F1 score는 끝납니다. 하지만 여전히 머릿속에 들어오진 않습니다. 더 복잡해지는 것 같기도 하네요. 하지만 다른 건 다 모르겠어도  `Precision`과 `Recall`이 F1 score와 관련이 있다는 건 알겠습니다. 



## 1. TP, TN, FP, FN

 `Precision`은 우리말로 **정밀도**라고도 합니다.  `Recall`의 경우 **재현율**이라고도 부릅니다.  앞에서도 한번 말했지만 이미 까먹으셨을 것 같아 다시 한 번 썼습니다. 

처음에는 의아하게 생각할 수도 있습니다. 답이 맞느냐 안 맞느냐만 따지면 되는 거 아닌가? 무슨 기준이 이렇게 많아. 그 이유는 현실 세계가 너무 복잡하게 생겨먹었기 때문입니다. 간단한 예시를 들어볼까요. 

당장 내일 비가 올지 오지 않을지 궁금해하는 상황이라고 해봅시다. 

![Ds_3TUXXgAARYm0](https://user-images.githubusercontent.com/58945760/81881467-334ff800-95cb-11ea-85d6-293b880cd298.jpg) 

 여기서 우리가 풀어야 할 문제는 **분류(classification)**로 판단하느냐 **회귀(regression)**으로 판단하느냐에 따라 달라질 수 있습니다. 분류 문제로 볼 경우, 내일 날씨가 비가 오느냐 오지 않느냐로 분류하는 이진 분류 문제(binary classification)가 될 수도 있고, 맑음, 흐림, 천둥번개와 같은 날씨 분류 클래스의 수를 더 늘린다면 다중 분류 문제(multiple classification)가 될 수도 있을 것입니다. 여기서는 보다 간단한 이진 분류 문제라고 판단하고 예측 모델을 만들었다 가정하겠습니다. 그럼 이 모델의 성능을 어떻게 판단해야 할까요? 우선 모델 결과와 실제 결과가 같아야  하겠지요? 

예측 모델을 돌린 결과 비가 내릴 것이라고 했는데, 다음 날 정말로 비가 내렸다면 모델의 성능이 좋다고 평가할 수 있을 것입니다.  반대로 비가 내리지 않을 것이라고 예측했는데 실제로 비가 내리지 않았을 때도 마찬가지겠지요. 여기서 전자를 **검증 기준 1**, 후자를 **검증 기준 2**라고 해보겠습니다. 

매일 출근해야 하는 직장인,  혹은 대학생의 관점에서 어떤 기준이 더 중요하게 느껴질까요? *우산을 가져왔는데 비가 내리지 않는 것*과, *우산이 없는데 비가 내리는 상황*은 짊어져야 하는 리스크부터가 다릅니다. 대중교통을 이용하는데 회사가 역에서 멀다면? 가방 안에 중요한 문서가 든 노트북이 있었다면? 상사와 함께 참석하는 미팅이 있어 옷을 깔끔히 차려입었다면?



![캡처2](https://user-images.githubusercontent.com/58945760/81884035-c0964b00-95d1-11ea-990f-94672eb23462.PNG)

 <em>아 망했어요......</em>



비단 직장인뿐 아니라 **대부분의 사람들에게는 검증 기준 1의 결과가 2에 비해 더 중요할 것입니다.** 검증기준 2가 쓸모없다는 것은 아닙니다만, 적어도 마케팅에 있어서는 1보다 중요도가 떨어지는 것이죠. 왜 갑자기 마케팅이냐구요? 그럼 반문하겠습니다. 이 모델을 왜 만듭니까? 왜 모델의 성능을 올리려고 할까요? 당연히 팔기 위해서지요. 비싸게 팔아 보다 많은 돈을 벌기 위해서입니다.  그럼 어떻게 해야 더 비싼 값을 받고 이 모델을 팔 수 있을까요? 

고객들의 니즈와 상황을 철저히 분석하고, 이 모델이 바로 그들의 문제를 해결해줄 최고의 방법이라는 것을 효과적으로 광고해야지요.

요컨대, 고객들이나 투자자들에게 **프레젠테이션을 할 때 검증 기준 1을 적용한 결과가 우수하다는 사실을 알리는 것**이, 다른 기준을 적용한 결과의 우수성을 알리는 것보다 더 효과적이라는 말입니다. 개발 단계에서 모델 성능을 올릴  때 중점을 두어야 할 기준 역시 검증 기준 1이어야 할 테고요. 맑은 날에 귀찮게 우산을 챙기는 일 없이, 비 오는 날에 옷 안 버리고 소중한 노트북을 지키기 위해서 말입니다. 



보다 직접적인 다른 예를 들어보겠습니다. 암의 감염 여부를 분류할 수 있는 모델이 있다고 합시다. 



![31088735225_1235f681fe_b](https://user-images.githubusercontent.com/58945760/81888728-e9700d80-95dc-11ea-833c-96c1b19cd9d7.jpg)



*모델로 예측한 결과가 암인데 실제로도 암인 것*, *모델 결과가 암에 걸리지 않았다는 것인데 실제로도 암에 걸리지 않은 것* 중 무엇이 모델의 성능 평가에 있어 더 중요할까요? 

*모델이 암에 걸리지 않았다고 판단했는데 실제로는 암에 걸린 상황*과 그 반대의 상황 중에서는요?  

참고로 암 중에서도 악질이라고 불리는 췌장암의 치사율(사망률)은 90%에 육박합니다. **모델 개발에 있어 어떤 기준을 우선에 두느냐**에 따라 죽을 사람을 살릴 수도, 살 수 있었던 사람을 죽게 만들 수도 있는 것입니다. 

그렇다면 이러한 기준은 어떤 것이 있을까요? 우리 모델에 있어서는 어떤 기준을 적용하는 것이 미래의 고객들을 설득시킬 수 있을까요? 우선 아래의 그림을 봅시다.



![실제 정답 vs 분류 결과](https://user-images.githubusercontent.com/58945760/81886975-115d7200-95d9-11ea-9884-851421a73266.png)

분류 모델의 `output`과 실제 정답을 비교할 때는 네 가지 경우가 있을 수 있습니다. 위의 예시를 통해 구분해보면 아래와 같습니다.

 

예시 1. 내일 비가 내릴까요? (비가 내린다(1=True), 비가 내리지 않는다(0=False))

- True Positive(**TP**) : 모델은 비가 내린다고 예측(True)했고 실제로도 비가 내렸다(True).
- False Positive(**FP**) : 모델은 비가 내린다고 예측(True)했고 실제로는 비가 내리지 않았다(False).
- True Negative(**TN**) : 모델은 비가 내리지 않는다고 예측(False)했고, 실제로도 비가 내리지 않았다(False).  
- False Negative(**FN**) : 모델은 비가 내리지 않는다고 예측(False)했고, 실제로는 비가 내렸다(True).



처음 봤을 때는 헷갈리기 쉽습니다. 하지만 정확한 위치를 알면 기억하기가 한결 수월해질 것입니다. 다른 예시를 봅시다. 



예시 2. 내가 암에 걸렸나요? (암에 걸렸다(1=True), 암에 걸리지 않았다(0=False))

- True Positive(**TP**) : 모델은 내가 암에 걸렸다고 예측(True)했고 실제로 암에 걸렸다(True).
- True Negative(**TN**) : 모델은 내가 암에 걸리지 않았다고 예측(False)했고, 실제로도 암에 걸리지 않았다(False).  
- False Positive(**FP**) : 모델은 내가 암에 걸렸다고 예측(True)했지만 실제로는 암에 걸리지 않았다(False).
- False Negative(**FN**) : 모델은 내가 암에 걸리지 않았다고 예측(False)했지만, 실제로는 암에 걸렸다(True).



차이가 보이시나요? 앞부분에는 True 혹은 False가 들어갑니다. 먼저 앞부분의 의미는 이렇습니다.  



- True :  모델의 결과와 실제 결과가 일치함 

- False : 모델의 결과와 실제 결과가 일치하지 않음

  

뒷부분에는 Positive 혹은 Negative가 들어갑니다. 



- Positive : 모델의 예측 결과가 True일 때 
- Negative : 모델의 예측 결과가 False일 때



이걸 풀어서 설명하면 아래와 같이 됩니다. 



- True Positive(**TP**) :  모델 예측 결과와 실제 답이 일치하고, 모델 예측 결과가 True이다.
- True Negative(**TN**) : 모델 예측 결과와 실제 답이 일치하고, 모델 예측 결과가 False이다.
- False Positive(**FP**) :  모델 예측 결과와 실제 답이 다르고, 모델 예측 결과가 True이다.
- False Negative(**FN**) : 모델 예측 결과와 실제 답이 다르고, 모델 예측 결과가 False이다.



더 쉽게 풀어보자면 이런 느낌? 



- True Positive(**TP**) :  모델 예측 결과와 실제 답이 일치하고, 모델 예측 결과가 True이다.

  ​								= 맞다고 했는데 맞았음

- True Negative(**TN**) : 모델 예측 결과와 실제 답이 일치하고, 모델 예측 결과가 False이다.

  ​								= 아니라고 했는데 맞았음

- False Positive(**FP**) :  모델 예측 결과와 실제 답이 다르고, 모델 예측 결과가 True이다.

  ​								= 맞다고 했는데 틀렸음

- False Negative(**FN**) : 모델 예측 결과와 실제 답이 다르고, 모델 예측 결과가 False이다. 

  ​								= 아니라고 했는데 틀렸음



여기에서 의문이 생길 수 있습니다. 이진 분류야 그렇다고 쳐요. 하지만 다중 분류에서 이걸 어떻게 측정할 수 있나요? 맞다 아니다를 판단하는 게 아니라 어떤 분류에 속하냐를 판단하는 거잖아요. 일부 이진분류도 마찬가지일 것 같은데.

맞습니다. 그래서 모델의 성능을 평가할 때는 클래스별로 이 네 가지(TP, TN, FP, FN) 지표를 측정합니다. 만약 내일 날씨를 예상하는 모델의 분류 클래스가 맑음, 비, 흐림, 천둥번개라면 총 16번을 측정한다는 것입니다. 맑음을 예시로 들어볼까요?

 

- 맑음 클래스의 True Positive(**TP**) :  모델 예측 결과와 실제 날씨가 일치했고, 모델 예측 결과가 맑음이었다. 
- 맑음 클래스의 True Negative(**TN**) : 모델 예측 결과와 실제 날씨가 일치했고, 모델 예측 결과가 맑음이 아니었다. 
- 맑음 클래스의 False Positive(**FP**) :  모델 예측 결과와 실제 날씨가 달랐고, 모델 예측 결과가 맑음이었다.
- 맑음 클래스의 False Negative(**FN**) : 모델 예측 결과와 실제 날씨가 달랐고, 모델 예측 결과가 맑음이 아니었다.  



이제 TP, TN, FP, FN의 개념적인 부분에 대해서는 어느 정도 이해가 되셨을 것입니다. 그럼 또 다른 의문이 떠오를지도 모르겠습니다. 이 네 가지 지표만으로도 모델의 성능은 충분히 판단할 수 있을 것 같은데, `Recall`과 `Precision`은 왜 필요한가? 그 부분은 다음 순서에서 알아보겠습니다. 



## 2. 정밀도(Precision)와 재현율(Recall)

![unnamed](https://user-images.githubusercontent.com/58945760/81892149-512a5680-95e5-11ea-8257-9656bcc3c99f.jpg)

 `Precision`은 우리말로 **정밀도**라고도 합니다.  `Recall`의 경우 **재현율**이라고도 부릅니다.  앞에서도 한번 말했지만 이미 까먹으셨을 것 같아 다시 한 번 썼습니다. 이제 조금은 식을 이해할 수 있을 것 같으니 차근차근 풀어 해석해봅시다.  정확도(accuracy, acc)는 일단 넘어가겠습니다. 

먼저 정밀도에 대해 알아봅시다. 이해가 쉽도록, 위에서 봤던 암 판정 모델의 예시로 식을 풀어보도록 하겠습니다.



![정밀도 예시](https://user-images.githubusercontent.com/58945760/81895605-9b173a80-95ed-11ea-9d40-f1666f524758.PNG)



분모를 자세히 들여다보니 공통점이 보이네요. 위에서 Positive는 모델이 True라고 예측한 것을 뜻한다고 말씀드렸습니다. 즉, 정밀도의 분모는 **모델이 암이라고 판정한 모든 경우의 수**입니다. 모델이 암으로 판단했을 때, 그 중에서 얼마나 암환자를 잘 맞추어냈는가를 정밀하게 나타내는 지표라고 볼 수 있겠습니다. 



이번에는 재현율을 살펴보겠습니다. 똑같이 암 판정 모델을 예시로 들겠습니다. 



![재현율 예시](https://user-images.githubusercontent.com/58945760/81896268-35c44900-95ef-11ea-9124-1c85bfc3508a.PNG)



이번에도 공통점이 보이네요. 정밀도의 분모가 모델이 암이라고 판정한 전체 경우의 수라면, 재현율의 분모는 **실제로 암인 전체 경우의 수**입니다. 실제 암환자 중 모델이 암이라고 판단한 비율은 얼마나 되는가를 나타내는 지표로,  학습된 모델이 현실에서도 학습만큼의 결과를 재현할 수 있는가를 나타내는 지표라 하겠습니다. 