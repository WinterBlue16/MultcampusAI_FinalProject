# Model 검증 방법(F1 score)

> 감정 분류 모델을 검증하기 위한 방법 중 하나인 F1 score에 대해 정리합니다. F1 score를 구하는 식은 아래와 같습니다.



## 0. 들어가기

 앞서 말했듯이 F1 score는 분류 모델, classification 문제를 풀기 위한 모델을 평가하는 지표입니다. 우리의 모델은 얼굴 이미지를 받아들여 감정을 분류하는 모델로써, `input` 데이터를 기쁨(positive)/무표정(neutral)/찡그림(negative) 세 가지 중 하나로 분류한 `output`을 내보냅니다. 

 이러한 모델을 검증하기 위해서는 어떤 방법이 필요할까요? F1 score는 어떤 것이길래 이 모델의 성능을 측정할 수 있다는 것일까요? 우선 `F1 score`의 식부터 확인해 보겠습니다.  

![C13314_01_33](https://user-images.githubusercontent.com/58945760/81763078-4dc19d00-9509-11ea-99a5-0bf86fe30435.jpg)



 당연히 이해가 안 됩니다. 당최 어떻게 모델의 성능을 측정할 수 있다는 것인지 머릿속에 들어오질 않습니다.

 그래도 곱셈과 나눗셈은 알겠는데, 도무지 의미를 알 수 없는 저 알파벳들이 문제입니다.  `Precision`은 뭐고 `Recall`은 무엇일까요? 확실한 건, F1 score가 뭔지 알기 위해서는 이 두 가지를 이해해야 한다는 것입니다. 그럼 차근 차근 가보도록 하겠습니다. 아래의 그림을 봐 주세요.



![전체 식](https://user-images.githubusercontent.com/58945760/81879352-7b6c1c00-95c5-11ea-86b4-4c2ed11df9f5.png)

 사실 이 그림 한 장만 이해하면 F1 score는 끝납니다. 하지만 여전히 머릿속에 들어오진 않습니다. 더 복잡해지는 것 같기도 하네요. 하지만 다른 건 다 모르겠어도 `Recall`과 `Precision`이 `F1 score`와 관련이 있다는 건 알겠습니다. 



## 1. 재현율(Recall)과 정밀도(Precision)

 `Recall`은 우리말로 **재현율**이라고도 합니다. `Precision`의 경우 **정밀도**라고도 부릅니다. 두 가지 모두 모델이 얼마나 정확한가를 따지기 위한 기준입니다. 

처음에는 의아하게 생각할 수도 있습니다. 답이 맞느냐 안 맞느냐만 따지면 되는 거 아닌가? 무슨 기준이 이렇게 많아. 그 이유는 현실 세계가 너무 복잡하게 생겨먹었기 때문입니다. 간단한 예시를 들어볼까요. 

당장 내일 비가 올지 오지 않을지 궁금해하는 상황이라고 해봅시다. 

![Ds_3TUXXgAARYm0](https://user-images.githubusercontent.com/58945760/81881467-334ff800-95cb-11ea-85d6-293b880cd298.jpg) 

 여기서 우리가 풀어야 할 문제는 **분류(classification)**로 판단하느냐 **회귀(regression)**으로 판단하느냐에 따라 달라질 수 있습니다. 분류 문제로 볼 경우, 내일 날씨가 비가 오느냐 오지 않느냐로 분류하는 이진 분류 문제(binary classification)가 될 수도 있고, 맑음, 흐림, 천둥번개와 같은 날씨 분류 클래스의 수를 더 늘린다면 다중 분류 문제(multiple classification)가 될 수도 있을 것입니다. 여기서는 보다 간단한 이진 분류 문제라고 판단하고 예측 모델을 만들었다 가정하겠습니다. 그럼 이 모델의 성능을 어떻게 판단해야 할까요? 우선 모델 결과와 실제 결과가 같아야  하겠지요? 

예측 모델을 돌린 결과 비가 내릴 것이라고 했는데, 다음 날 정말로 비가 내렸다면 모델의 성능이 좋다고 평가할 수 있을 것입니다.  반대로 비가 내리지 않을 것이라고 예측했는데 실제로 비가 내리지 않았을 때도 마찬가지겠지요. 여기서 전자를 **검증 기준 1**, 후자를 **검증 기준 2**라고 해보겠습니다. 

매일 출근해야 하는 직장인,  혹은 대학생의 관점에서 어떤 기준이 더 중요하게 느껴질까요? *우산을 가져왔는데 비가 내리지 않는 것*과, *우산이 없는데 비가 내리는 상황*은 짊어져야 하는 리스크부터가 다릅니다. 대중교통을 이용하는데 회사가 역에서 멀다면? 가방 안에 중요한 문서가 든 노트북이 있었다면? 상사와 함께 참석하는 미팅이 있어 옷을 깔끔히 차려입었다면?



![캡처2](https://user-images.githubusercontent.com/58945760/81884035-c0964b00-95d1-11ea-990f-94672eb23462.PNG)

 <em>아 망했어요......</em>



비단 직장인뿐 아니라 **대부분의 사람들에게는 검증 기준 1의 결과가 2에 비해 더 중요할 것입니다.** 검증기준 2가 쓸모없다는 것은 아닙니다만, 적어도 마케팅에 있어서는 1보다 중요도가 떨어지는 것이죠. 왜 갑자기 마케팅이냐구요? 그럼 반문하겠습니다. 이 모델을 왜 만듭니까? 왜 모델의 성능을 올리려고 할까요? 당연히 팔기 위해서지요. 비싸게 팔아 보다 많은 돈을 벌기 위해서입니다.  그럼 어떻게 해야 더 비싼 값을 받고 이 모델을 팔 수 있을까요? 

고객들의 니즈와 상황을 철저히 분석하고, 이 모델이 바로 그들의 문제를 해결해줄 최고의 방법이라는 것을 효과적으로 광고해야지요.

요컨대, 고객들이나 투자자들에게 **프레젠테이션을 할 때 검증 기준 1을 적용한 결과가 우수하다는 사실을 알리는 것**이, 다른 기준을 적용한 결과의 우수성을 알리는 것보다 더 효과적이라는 말입니다. 개발 단계에서 모델 성능을 올릴  때 중점을 두어야 할 기준 역시 검증 기준 1이어야 할 테고요. 맑은 날에 귀찮게 우산을 챙기는 일 없이, 비 오는 날에 옷 안 버리고 소중한 노트북을 지키기 위해서 말입니다. 

보다 직접적인 다른 예를 들어보겠습니다. 암의 감염 여부를 분류할 수 있는 모델이 있다고 합시다. 





*모델로 예측한 결과가 암인데 실제로도 암인 것*, *모델 결과가 암에 걸리지 않았다는 것인데 실제로도 암에 걸리지 않은 것* 중 무엇이 모델의 성능 평가에 있어 더 중요할까요? *모델이 암에 걸리지 않았다고 판단했는데 실제로는 암에 걸린 상황*과 그 반대의 상황 중에서는요?  참고로 암 중에서도 악질이라고 불리는 췌장암의 치사율(사망률)은 90%에 육박합니다. 



![실제 정답 vs 분류 결과](https://user-images.githubusercontent.com/58945760/81886975-115d7200-95d9-11ea-9884-851421a73266.png)

